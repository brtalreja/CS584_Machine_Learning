{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7_hx8SJJR4-"
   },
   "source": [
    "# Assignment 2 - Logistic Regression & Naive Bayes\n",
    "\n",
    "Due by 11:59pm, Feb 24, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyGrk2X_J_1h"
   },
   "source": [
    "## Theory Questions (Full points: 40, each question 4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8OnMEI0JXxA"
   },
   "source": [
    "1. Explain the importance of setting up learning rate in the gradient descent based methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv3iz5dBJX7Z"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO8kErgCJX-P"
   },
   "source": [
    "2. What is the stochastic gradient descent? Why do we need stochastic gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEviYAL2JeWs"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vQD3sgwJYBS"
   },
   "source": [
    "3. Explain the reasons to perform feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-e37RdDJfmd"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOqhvEpuJYEX"
   },
   "source": [
    "4. What is the probabilistic generative model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf5JjDFDJe6G"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8V802XDJjxe"
   },
   "source": [
    "5. Explain how we perform maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3PrgB7kJl99"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kUcM98LJj9N"
   },
   "source": [
    "6. Explain the reasons about using cross entropy loss in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuECRF2PJk81"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8bCRVRaJkBP"
   },
   "source": [
    "7. Explain the differences between discriminative and generative model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEdAdRiu1ksO"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcZpkIc01Rwe"
   },
   "source": [
    "8. What is N-folds? Explain the reasons why we need N-folds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2KIVplzJlYt"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCL0MZN6JkET"
   },
   "source": [
    "9. Bishop's Book \"Pattern Recognition and Machine learning\" - Exercise 4.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-h519EbJmbz"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_JyVbXB0LSH"
   },
   "source": [
    "10. Bishop's Book \"Pattern Recognition and Machine learning\" - Exercise 4.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JYA083y0Z7Q"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AiWTVDf7cZ2"
   },
   "source": [
    "## Programming Questions (Full points: 60, each question 30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hUXdV3L376cA"
   },
   "outputs": [],
   "source": [
    "# Do not edit the codes in this cell\n",
    "# load required library\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "# load dataset\n",
    "x, y = load_wine(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iwaee_T-Urj"
   },
   "source": [
    "### - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyrt2Ka7VFYK"
   },
   "source": [
    "0. train_test_split (Done in the below cell)\n",
    "\n",
    "We **randomly** split data into train and test set. The number of training data and testing data is 100 and 30, respectively. Do not modify the split data. You need to use training data to train your model and obtain an optimal solution. Finally, using your model with the optimal solution to predict the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9M1DAu9XPY8E"
   },
   "outputs": [],
   "source": [
    "# Do not edit the codes in this cell\n",
    "# We split train and test data for logistic regression function\n",
    "\n",
    "test_lists = [0, 4, 5, 7, 13, 15, 19, 27, 28, 30, 31, 34, 39, 47, 63, 74, 78, 83, 90, 92, 95, 97, 103, 113, 119, 122, 123, 125, 126, 127]\n",
    "\n",
    "x = x[:130, :]\n",
    "y = y[:130]\n",
    "\n",
    "# training data: you can use it to training your model.\n",
    "train_x = np.array([sub_x for index, sub_x in enumerate(x) if index not in test_lists])\n",
    "train_y = np.array([sub_y for index, sub_y in enumerate(y) if index not in test_lists])\n",
    "\n",
    "# testing data: ONLY use it to measure your model, do NOT use during training.\n",
    "test_x = np.array([sub_x for index, sub_x in enumerate(x) if index in test_lists])\n",
    "test_y = np.array([sub_y for index, sub_y in enumerate(y) if index in test_lists])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LdQ5yubLJld"
   },
   "source": [
    "In the assignment 2, you have more freedom on your programming design. In this part, you are going to implement your own Logistic Regression function. You need to implement logistic regression with stochastic gradient descent from scratch. The required functions are listed below. You can add more functions as you need. **No library versions of logistic regression are allowed**. \n",
    "_________\n",
    "\n",
    "1. train_val_split\n",
    "\n",
    "**Randomly** split training data into train and val set. 80% of the training data will be the train set and 20% of the training data will be the val set.\n",
    "\n",
    "2. normalization (data preprocessing)\n",
    "\n",
    "You should normalize all data for each attribute firstly. \n",
    "\n",
    "3. sigmoid\n",
    "\n",
    "The core of logistic regression\n",
    "\n",
    "4. predict\n",
    "\n",
    "Predict an output value for a given x with a set of coefficients. \n",
    "\n",
    "5. accurate\n",
    "\n",
    "Calculate accuracy percentage of the predictions.\n",
    "\n",
    "6. coef_gd\n",
    "\n",
    "Estimate logistic regression coefficients using **vanilla gradient descent**. Using **the cross entropy loss**. Carefully choose learning rate and epochs values.\n",
    "\n",
    "7. draw_model\n",
    "\n",
    "a) Plot the both training loss and validation loss for each epochs.\n",
    "\n",
    "b) Plot the both training accuracy and validation accuracy for each epochs. \n",
    "\n",
    "8. predict the testing data\n",
    "\n",
    "Use your pre-trained model to predict the testing data. Print out your **testing accurate**. Is it good? If not, analyze the reason in short and modify your code to improve.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AHamaj3f5xY"
   },
   "outputs": [],
   "source": [
    "#\n",
    "def draw_data(x,y):\n",
    "    plt.scatter(x[:50,0],x[:50,1],marker='x',c='b',label ='0')\n",
    "    plt.scatter(x[50:,0],x[50:,1],marker='o',c='g',label ='1')\n",
    "    plt.title(\"Scatter Plot of data\")\n",
    "    plt.xlabel(\"x Value\")\n",
    "    plt.ylabel(\"y Value\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def sigmoid(n):\n",
    "    sig = 1 / (1 + np.exp(-n))\n",
    "    return sig\n",
    "def cost(theta, x, y):\n",
    "    k = len(x)\n",
    "    sig = sigmoid(np.dot(x,theta))\n",
    "    class1 = -y*np.log(sig)\n",
    "    class2 = (1-y)*np.log(1-sig)\n",
    "    cost = (class1 - class2)\n",
    "    cost_cal = cost.sum()/k\n",
    "    return cost_cal\n",
    "def gradient(theta, x, y):\n",
    "    learning_rate = 0.01\n",
    "    n = len(x)\n",
    "    yhat = sigmoid(np.dot(x,theta))\n",
    "    gradient = np.dot(x.T,  yhat - y)\n",
    "    gradient /= n\n",
    "    gradient *= learning_rate\n",
    "    theta -= gradient\n",
    "    return theta\n",
    "def predict(theta, x):\n",
    "    sig2 = sigmoid(np.dot(x,theta))\n",
    "    for i in range(len(sig2)):\n",
    "        if sig2[i] >= 0.5:\n",
    "            sig2[i] = 1\n",
    "        else:\n",
    "            sig2[i] = 0\n",
    "    return sig2\n",
    "def accurate(predictions, y):\n",
    "    differnce = predictions - y\n",
    "    accuScore = 1.0 - (float(np.count_nonzero(differnce)) / len(differnce))\n",
    "    return accuScore\n",
    "def decision_boundary(x,y,theta):\n",
    "    plt.scatter(x[:50,0],x[:50,1],marker='x',c='b',label ='0')\n",
    "    plt.scatter(x[50:,0],x[50:,1],marker='o',c='g',label ='1')\n",
    "    X = np.arange(0,9,1)\n",
    "    Y = -(theta[0]*X) / theta[1]\n",
    "    plt.plot(X,Y,'-g',label=\"Decision Boundary\")\n",
    "    plt.title(\"Decision boundary\")\n",
    "    plt.xlabel(\"X Values\")\n",
    "    plt.ylabel(\"Y Values\")\n",
    "    plt.xlim(0,8)\n",
    "    plt.ylim(0,5)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# initialize weight\n",
    "theta = np.zeros(x_lr.shape[1])\n",
    "draw_data(x_lr,y_lr)\n",
    "epochs = 2000\n",
    "for i in range(epochs):\n",
    "  cost_fun = cost(theta, x_lr, y_lr)\n",
    "  theta = gradient(theta, x_lr, y_lr)\n",
    "print(\"The Final Cost is:\",cost_fun)\n",
    "predictions = predict(theta,x_lr)\n",
    "score = accurate(predictions,y_lr)\n",
    "print(\"The Accuracy Score is:\",score)\n",
    "decision_boundary(x_lr,y_lr,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eNsYoFL7pOG"
   },
   "source": [
    "### - Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTuXPz9V8jIA"
   },
   "source": [
    "In this part, you are going to implement your own naive bayes classifier. \n",
    "You need to implement naive bayes classifier from scratch **using the training data of wine datasets train_x and train_y**. The required functions are listed below. You can add more functions as you need. **No library versions of naive bayes classifier are allowed**. \n",
    "_________\n",
    "\n",
    "1. train_val_split\n",
    "\n",
    "**Randomly** split training data into train and val set. 80% of the training data will be the train set and 20% of the training data will be the val set.\n",
    "\n",
    "2. normalization (data preprocessing)\n",
    "\n",
    "You should normalize all data for each attribute firstly. \n",
    "\n",
    "3. cross_validation_split\n",
    "\n",
    "**Randomly** split data into 5 folds.\n",
    "\n",
    "\n",
    "4. predict\n",
    "\n",
    "Predict the class label for a given x. \n",
    "\n",
    "5. accurate\n",
    "\n",
    "Calculate accuracy percentage of the predictions. Remember to average the results of k folds\n",
    "\n",
    "6. gaussian_probability\n",
    "\n",
    "Calculate the Gaussian probability distribution function for the given x. \n",
    "\n",
    "![Image_20210920015911.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    "5. class_probability\n",
    "\n",
    "Calculate the probabilities of predicting each class for a given x using naive bayes algorithm. Be aware that we have multiple input variables. And you may want to use gaussian_probability function here.\n",
    "\n",
    "*Print out your accuracy result. Is it good? If not, analyze the reason in short. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-jO9XrADS92"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
