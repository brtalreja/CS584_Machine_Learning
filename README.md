# CS584: Machine Learning (Spring 2023, Guidance: Prof. Yan Yan)
Illinois Institute of Technology

This repository contains my learning journey in the subject of Machine Learning. The course covered a variety of topics to equip me with the fundamental skills in advanced machine learning methodologies. Below is a summary of the key concepts and techniques learned:

## Table of Contents

- Introduction to Machine Learning
- Regression
- Classification
- Convolutional Neural Networks (CNN)
- Recurrent Neural Networks (RNN)
- Deep Neural Networks (DNN)
- Transfer Learning

### Introduction to Machine Learning

The course began with an introduction to the field of machine learning, providing insights into its applications and importance in extracting valuable information from large datasets.

### Regression

Mastered the concept of regression, a fundamental technique for predicting numerical outcomes based on input features. Topics covered included linear regression, polynomial regression, and regularization techniques such as Ridge and Lasso regression.

### Classification

Explored classification algorithms, their applications, and the process of training models to categorize data into predefined classes. Techniques covered included logistic regression, support vector machines (SVM), k-nearest neighbors (KNN), and decision trees.

### Convolutional Neural Networks (CNN)

Studied the architecture and applications of Convolutional Neural Networks (CNNs) in image recognition and computer vision. Topics included convolutional layers, pooling layers, activation functions, and popular CNN architectures like VGG, ResNet, and Inception.

### Recurrent Neural Networks (RNN)

Explored the structure and use cases of Recurrent Neural Networks (RNNs) for sequential data analysis. Covered the basics of RNNs, Long Short-Term Memory (LSTM) networks, and Gated Recurrent Units (GRUs), focusing on their applications in natural language processing and time series prediction.

### Deep Neural Networks (DNN)

Gained an understanding of deep neural networks (DNNs), including their architecture, training processes, and applications. Topics included feedforward neural networks, backpropagation, gradient descent, and the importance of activation functions.

### Transfer Learning

Investigated transfer learning techniques to leverage pre-trained models for new tasks. Covered the benefits of transfer learning, common pre-trained models (e.g., VGG, ResNet), and practical applications in various domains.

---

This repository serves as a comprehensive record of my journey through the Machine Learning course, showcasing the skills and knowledge gained in various machine learning methodologies. Feel free to explore the code and documentation for a detailed insight into each topic.
